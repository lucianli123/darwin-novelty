{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3171cadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80e3f049",
   "metadata": {},
   "outputs": [],
   "source": [
    "bio=pd.read_csv(\"bio_auth_books.csv\")\n",
    "bio2=pd.read_csv(\"bio_auth_books2.csv\")\n",
    "bio=pd.concat((bio,bio2),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eca3954b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bio=bio.drop_duplicates(subset=[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b12f3423",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"desc.txt\", \"r\", encoding=\"utf-8\")\n",
    "text=f.read()\n",
    "f.close()\n",
    "text=text.replace('\\r', ' ').replace('\\n', ' ')\n",
    "text=text.split(\"PREFACE TO THE SECOND EDITION.\")[2]\n",
    "text=text.split(\"probability to the action of sexual selection.\")[0]\n",
    "text+=\"probability to the action of sexual selection.\"\n",
    "text=text.strip()\n",
    "dom = filt(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fe671087",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"oos.txt\", \"r\", encoding=\"utf-8\")\n",
    "text=f.read()\n",
    "f.close()\n",
    "text=text.replace(\"\\n\",\" \")\n",
    "text=text.split(\"Sixth London Edition, with all Additions and Corrections.\")[1]\n",
    "text=text.split(\"from so simple a beginning endless forms most beautiful and most wonderful have been, and are being evolved.\")[0]\n",
    "text=text.strip()\n",
    "oos = filt(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a98ee1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filt(sents):\n",
    "    ret=[]\n",
    "    for s in sents:\n",
    "        if len(s)>15:\n",
    "            ret.append(s)\n",
    "    return ret\n",
    "def clean(str):\n",
    "\n",
    "    text = str.replace('\\r', ' ').replace('\\n', ' ')\n",
    "    #text=re.sub(r'[^a-zA-Z. ]+', '', text)\n",
    "    text=\" \".join(text.split())\n",
    "    text=text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7f59a782",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for id in bio[\"id\"]:\n",
    "    path=glob.glob('bio/'+id+'/*djvu.txt')\n",
    "    try:\n",
    "        f = open(path[0], \"r\", encoding=\"utf-8\")\n",
    "        text=f.read()\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    if len(text)<1000:\n",
    "        continue\n",
    "    text=clean(text)\n",
    "    out = open(\"clean_bio/\"+id+\".txt\", \"w\", encoding=\"utf-8\")\n",
    "    out.write(text)\n",
    "    f.close() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99bda2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('thenlper/gte-small')\n",
    "import pickle\n",
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37073d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.util import cos_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f99cf4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8547]])\n"
     ]
    }
   ],
   "source": [
    "sentences = ['I at least have so much to do in unraveling certain human lots, and seeing how they were woven and interwoven, that all the light I can command must be concentrated on this particular web, and not dispersed over that tempting range of relevancies called the universe.', 'We shall never, probably, disentangle the inextricable web of affinities between the members of any one class; but when we have a distinct object in view, and do not look to some unknown plan of creation, we may hope to make sure but slow progress.']\n",
    "\n",
    "embeddings = model.encode(sentences)\n",
    "print(cos_sim(embeddings[0], embeddings[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dc2d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('vec_oos.pickle', 'rb')\n",
    "emb_oos = pickle.load(file)\n",
    "file.close()\n",
    "file = open('vec_dom.pickle', 'rb')\n",
    "emb_dom = pickle.load(file)\n",
    "file.close()\n",
    "emb_oos=np.array(emb_oos)\n",
    "emb_dom=np.array(emb_dom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fb2c678",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a267afec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for id in bio[\"id\"]:\n",
    "    if os.path.exists('bio_vectors/'+id+'.pickle'):\n",
    "        continue\n",
    "    path=glob.glob('clean_bio/'+id+'.txt')\n",
    "    try:\n",
    "        f = open(path[0], \"r\", encoding=\"utf-8\")\n",
    "        text=f.read()\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "\n",
    "    f.close()\n",
    "    sents = filt(sent_tokenize(text))\n",
    "    emb = model.encode(sents) \n",
    "    emb=np.array(emb)\n",
    "    \n",
    "    with open('bio_vectors/'+id+'.pickle', 'wb') as handle:\n",
    "        pickle.dump(emb, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc4bbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for comp in [\"oos\", \"dom\"]:\n",
    "    if comp==\"oos\":\n",
    "        other=emb_oos\n",
    "    else:\n",
    "        other=emb_dom \n",
    "\n",
    "    match=cdist(emb, other, \"cosine\")\n",
    "    match=np.asarray((match<0.1)).nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5ffa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa153ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
